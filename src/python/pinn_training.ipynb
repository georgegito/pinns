{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Informed Neural Networks <br> F1 Car Front Wing Aerodymanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pinn import PINN\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"/Users/ggito/repos/pinns/data/\"\n",
    "points_filename = \"front_wing_points_final.csv\"\n",
    "norms_filename = \"front_wing_norms_final.csv\"\n",
    "\n",
    "out_dir = \"/Users/ggito/repos/pinns/data/\"\n",
    "models_out_dir = \"/Users/ggito/repos/pinns/data/models/\"\n",
    "log_filepath = out_dir + \"log/log.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "  device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  print(\"GPU device not found.\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wing_df = pd.read_csv(in_dir + points_filename)\n",
    "# norm_df = pd.read_csv(in_dir + norms_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "output_dim = 4\n",
    "hidden_units = [2058, 2058, 2058]\n",
    "\n",
    "pinn = PINN(input_dim, output_dim, hidden_units).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.LBFGS(pinn.parameters())\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "Nf = 20000   # num of collocation points -> pde evaluation\n",
    "N0 = 20000   # num of points to evaluate initial conditons\n",
    "Nb = 20000   # num of points to evaluate boundary conditions\n",
    "Nw = 20000   # num of points of the surface of the front wing to evaluate boundary conditions\n",
    "\n",
    "# Density (rho): 1.2041kg/m^3\n",
    "# Dynamic viscosity (mu): 1.81e-5 kg/m.s\n",
    "rho = 1.2\n",
    "mu = 1.81e-5\n",
    "\n",
    "# m/s\n",
    "in_velocity = 20\n",
    "\n",
    "# Domain limits\n",
    "x_max = 1\n",
    "y_max = 1\n",
    "z_max = 1\n",
    "t_max = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_inputs(x_max, y_max, z_max, t_max, Nf, N0, Nb, Nw):\n",
    "  # TODO: use quasi monte carlo sampling\n",
    "  # collocation points\n",
    "  x_f = utils.tensor_from_array(utils.sample_points_in_domain(0, x_max, Nf), device=device, requires_grad=True)\n",
    "  y_f = utils.tensor_from_array(utils.sample_points_in_domain(0, y_max, Nf), device=device, requires_grad=True)\n",
    "  z_f = utils.tensor_from_array(utils.sample_points_in_domain(0, z_max, Nf), device=device, requires_grad=True)\n",
    "  t_f = utils.tensor_from_array(utils.sample_points_in_domain(0, t_max, Nf), device=device, requires_grad=True)\n",
    "  # xyzt_f = utils.stack_xyzt_tensors(x_f, y_f, z_f, t_f)\n",
    "  # if stacked in a single tensor, the gradients are not computed correctly\n",
    "\n",
    "  # initial condition points (t=0)\n",
    "  x0 = utils.tensor_from_array(utils.sample_points_in_domain(0, x_max, N0), device=device, requires_grad=False)\n",
    "  y0 = utils.tensor_from_array(utils.sample_points_in_domain(0, y_max, N0), device=device, requires_grad=False)\n",
    "  z0 = utils.tensor_from_array(utils.sample_points_in_domain(0, z_max, N0), device=device, requires_grad=False)\n",
    "  t0 = utils.tensor_from_array(utils.zeros(N0), device=device, requires_grad=False)\n",
    "  xyzt_0 = utils.stack_xyzt_tensors(x0, y0, z0, t0)\n",
    "\n",
    "  # boundary condition points (inflow, y=1)\n",
    "  x_b = utils.tensor_from_array(utils.sample_points_in_domain(0, x_max, Nb), device=device, requires_grad=False)\n",
    "  y_b = utils.tensor_from_array(utils.ones(Nb), device=device, requires_grad=False)\n",
    "  z_b = utils.tensor_from_array(utils.sample_points_in_domain(0, z_max, Nb), device=device, requires_grad=False)\n",
    "  t_b = utils.tensor_from_array(utils.sample_points_in_domain(0, t_max, Nb), device=device, requires_grad=False)\n",
    "  xyzt_b = utils.stack_xyzt_tensors(x_b, y_b, z_b, t_b)\n",
    "\n",
    "  # points & normal vectors on the surface of the wing\n",
    "  ## sample Nw wing points with the corresponding normals\n",
    "  sampled_indices = wing_df.sample(n=Nw).index\n",
    "\n",
    "  x_w, y_w, z_w = [utils.tensor_from_array(wing_df.loc[sampled_indices, col].values, device=device, requires_grad=False) for col in ['x', 'y', 'z']]\n",
    "  # n_x, n_y, n_z = [utils.tensor_from_array(norm_df.loc[sampled_indices, col].values, device=device, requires_grad=False) for col in ['x', 'y', 'z']]\n",
    "  t_w = utils.tensor_from_array(utils.sample_points_in_domain(0, t_max, Nw), device=device, requires_grad=False)\n",
    "\n",
    "  xyzt_w = utils.stack_xyzt_tensors(x_w, y_w, z_w, t_w)\n",
    "  # n_xyz = utils.stack_xyz_tensors(n_x, n_y, n_z)\n",
    "\n",
    "  return (x_f, y_f, z_f, t_f, xyzt_0, xyzt_b, xyzt_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "\n",
    "  global epoch_loss\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  training_input = create_training_inputs(x_max, y_max, z_max, t_max, Nf, N0, Nb, Nw)\n",
    "\n",
    "  total_loss, pde_loss, ic_loss, bc_loss, no_slip_loss = pinn.loss(\n",
    "                  *training_input,\n",
    "                  in_velocity,\n",
    "                  mu, rho,\n",
    "                  c1=1, c2=1, c3=1, c4=1)\n",
    "\n",
    "  epoch_loss = [total_loss.item(), pde_loss.item(), ic_loss.item(), bc_loss.item(), no_slip_loss.item()]\n",
    "\n",
    "  total_loss.backward()\n",
    "\n",
    "  return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss = []\n",
    "checkpoint_epochs = 5\n",
    "filename = \"v2.pt\"\n",
    "\n",
    "epoch = 1\n",
    "while epoch <= epochs:\n",
    "  optimizer.step(lambda: closure())\n",
    "\n",
    "  utils.save_log(log_filepath, epoch, epoch_loss)\n",
    "  utils.print_log(epoch, epoch_loss)\n",
    "\n",
    "  if np.isnan(epoch_loss[0]):\n",
    "    print(\"=> NaN loss...\")\n",
    "    pinn, optimizer, checkpoint_epoch = utils.load_checkpoint(pinn, optimizer, models_out_dir + filename)\n",
    "    epoch = checkpoint_epoch + 1\n",
    "    continue\n",
    "\n",
    "  if epoch % checkpoint_epochs == 0:\n",
    "    utils.save_checkpoint(pinn, epoch, optimizer, models_out_dir + filename)\n",
    "\n",
    "  epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinn, optimizer, checkpoint_epoch = utils.load_checkpoint(pinn, optimizer, models_out_dir + filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
